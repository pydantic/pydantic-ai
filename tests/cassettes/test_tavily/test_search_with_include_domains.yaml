interactions:
- request:
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate, br, zstd
      connection:
      - keep-alive
      content-length:
      - '117'
      content-type:
      - application/json
      host:
      - api.tavily.com
    method: POST
    parsed_body:
      include_domains:
      - arxiv.org
      query: transformer architectures
      search_depth: basic
      topic: general
    uri: https://api.tavily.com/search
  response:
    headers:
      connection:
      - keep-alive
      content-length:
      - '6906'
      content-security-policy:
      - default-src 'none'; script-src 'self'; connect-src 'self'; img-src 'self'; style-src 'self';base-uri 'self';form-action
        'self'; require-trusted-types-for 'script'; upgrade-insecure-requests;
      content-type:
      - application/json
    parsed_body:
      answer: null
      follow_up_questions: null
      images: []
      query: transformer architectures
      request_id: 5219ceb5-6bd9-4d38-b675-2fb7b8a887f2
      response_time: 1.08
      results:
      - content: '# Title:The Power of Architecture: Deep Dive into Transformer Architectures for Long-Term Time Series Forecasting.
          Authors:Lefei Shen, Mouxiang Chen, Han Fu, Xiaoxue Ren, Xiaoyun Joy Wang, Jianling Sun, Zhuo Li, Chenghao Liu. View
          a PDF of the paper titled The Power of Architecture: Deep Dive into Transformer Architectures for Long-Term Time
          Series Forecasting, by Lefei Shen and 7 other authors. View a PDF of the paper titled The Power of Architecture:
          Deep Dive into Transformer Architectures for Long-Term Time Series Forecasting, by Lefei Shen and 7 other authors.
          > Abstract:Transformer-based models have recently become dominant in Long-term Time Series Forecasting (LTSF), yet
          the variations in their architecture, such as encoder-only, encoder-decoder, and decoder-only designs, raise a crucial
          question: What Transformer architecture works best for LTSF tasks? | Cite as: | arXiv:2507.13043 [cs.LG] |. |  |
          (or  arXiv:2507.13043v1 [cs.LG] for this version) |. # Bibliographic and Citation Tools. Have an idea for a project
          that will add value for arXiv''s community?'
        raw_content: null
        score: 0.7923522
        title: Deep Dive into Transformer Architectures for Long-Term ...
        url: https://arxiv.org/abs/2507.13043
      - content: '# Computer Science > Machine Learning. # Title:Optimal Control for Transformer Architectures: Enhancing
          Generalization, Robustness and Efficiency. | Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI);
          Optimization and Control (math.OC) |. | Cite as: | arXiv:2505.13499 [cs.LG] |. |  | (or  arXiv:2505.13499v2 [cs.LG]
          for this version) |. |  |  Focus to learn more  arXiv-issued DOI via DataCite |. ### References & Citations. ##
          BibTeX formatted citation. # Bibliographic and Citation Tools. # Code, Data and Media Associated with this Article.
          # Recommenders and Search Tools. # arXivLabs: experimental projects with community collaborators. arXivLabs is a
          framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals
          and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence,
          and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have
          an idea for a project that will add value for arXiv''s community?'
        raw_content: null
        score: 0.783542
        title: '[2505.13499] Optimal Control for Transformer Architectures'
        url: https://arxiv.org/abs/2505.13499
      - content: 'Transformer-based models have recently become dominant in Long-term Time Series Forecasting (LTSF), yet
          the variations in their architecture, such as encoder-only, encoder-decoder, and decoder-only designs, raise a crucial
          question: What Transformer architecture works best for LTSF tasks? In recent years, Transformer-based models have
          become dominant in long-term time series forecasting (LTSF) tasks (Informer, ; Autoformer, ; FEDformer, ; PatchTST,
          ; iTransformer, ; TimeXer, ; ARMA\_Attention, ; Pyraformer, ; TFT, ; PDFormer, ; BasisFormer, ; SAMformer, ; Scaleformer,
          ; Quatformer, ), demonstrating strong performance across various real-world applications (TSF\_Energy\_1, ; TSF\_Energy\_2,
          ; TSF\_Economics\_1, ; TSF\_Web\_1, ; TSF\_Web\_2, ; TSF\_Web\_3, ; TSF\_Weather\_1, ; TSF\_Weather\_2, ; TSF\_Finance\_1,
          ). We examine Transformer-based LTSF models from multiple perspectives, including attention mechanisms, forecasting
          aggregation strategies, forecasting paradigms, and normalization layers. Based on the above conclusions, we construct
          an optimal Transformer architecture by combining the best choices, including bi-directional attention with joint-attention,
          complete forecasting aggregation, direct-mapping paradigm, and the BatchNorm layer.'
        raw_content: null
        score: 0.77731717
        title: Deep Dive into Transformer Architectures for Long-Term ...
        url: https://arxiv.org/html/2507.13043v1
      - content: '# Title:Lightweight Transformer Architectures for Edge Devices in Real-Time Applications. View a PDF of
          the paper titled Lightweight Transformer Architectures for Edge Devices in Real-Time Applications, by Hema Hariharan
          Samson. > Abstract:The deployment of transformer-based models on resource-constrained edge devices represents a
          critical challenge in enabling real-time artificial intelligence applications. This comprehensive survey examines
          lightweight transformer architectures specifically designed for edge deployment, analyzing recent advances in model
          compression, quantization, pruning, and knowledge distillation techniques. Experimental results demonstrate that
          modern lightweight transformers can achieve 75-96% of full-model accuracy while reducing model size by 4-10x and
          inference latency by 3-9x, enabling deployment on devices with as little as 2-5W power consumption. Comprehensive
          study of lightweight transformer architectures for edge computing with novel findings on memory-bandwidth tradeoffs,
          quantization strategies, and hardware-specific optimizations. |  | (or  arXiv:2601.03290v1 [cs.LG] for this version)
          |. View a PDF of the paper titled Lightweight Transformer Architectures for Edge Devices in Real-Time Applications,
          by Hema Hariharan Samson.'
        raw_content: null
        score: 0.76826453
        title: Lightweight Transformer Architectures for Edge Devices in ...
        url: https://www.arxiv.org/abs/2601.03290
      - content: '# Title:Study of Lightweight Transformer Architectures for Single-Channel Speech Enhancement. View a PDF
          of the paper titled Study of Lightweight Transformer Architectures for Single-Channel Speech Enhancement, by Haixin
          Zhao and Nilesh Madhu. Networks integrating stacked temporal and spectral modelling effectively leverage improved
          architectures such as transformers; however, they inevitably incur substantial computational complexity and model
          expansion. The proposed lightweight, causal, transformer-based architecture with adversarial training (LCT-GAN)
          yields SoTA performance on instrumental metrics among contemporary lightweight models, but with far less overhead.
          | Cite as: | arXiv:2505.21057 [eess.AS] |. |  | (or  arXiv:2505.21057v1 [eess.AS] for this version) |. View a PDF
          of the paper titled Study of Lightweight Transformer Architectures for Single-Channel Speech Enhancement, by Haixin
          Zhao and Nilesh Madhu. # Bibliographic and Citation Tools. arXiv is committed to these values and only works with
          partners that adhere to them. Have an idea for a project that will add value for arXiv''s community?'
        raw_content: null
        score: 0.7663815
        title: Study of Lightweight Transformer Architectures for Single ...
        url: https://arxiv.org/abs/2505.21057
    status:
      code: 200
      message: OK
version: 1
