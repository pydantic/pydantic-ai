"""OpenAI-compatible Pydantic models for request/response validation."""

from __future__ import annotations

from typing import Annotated, Any, Literal

try:
    from openai.types import ErrorObject, Reasoning
    from openai.types.chat import (
        ChatCompletionAudioParam,
        ChatCompletionMessageParam,
        ChatCompletionReasoningEffort,
        ChatCompletionToolParam,
    )
    from openai.types.chat.chat_completion_message_tool_call import ChatCompletionMessageToolCall
    from openai.types.model import Model
    from openai.types.responses import (
        ResponseIncludable,
        ResponseInputParam,
        ResponsePromptParam,
        ToolParam,
    )
except ImportError as _import_error:  # pragma: no cover
    raise ImportError(
        'Please install the `openai` and `fastapi` packages to enable the fastapi openai compatible endpoint, '
        'you can use the `chat-completion` optional group â€” `pip install "pydantic-ai-slim[chat-completion]"`'
    ) from _import_error
from pydantic import BaseModel, ConfigDict, Field

Number = Annotated[float, Field(ge=-2.0, le=2.0)]
PositiveNumber = Annotated[float, Field(ge=0.0, le=2.0)]


# ChatMessage example for docs
class ChatMessage(BaseModel):
    """Chat message model."""

    role: Literal['system', 'user', 'assistant', 'tool'] = Field(
        ...,
        description='Role of the message sender',
    )
    content: str = Field(..., description='Message content')

    tool_calls: list[ChatCompletionMessageToolCall] | None = None
    """The tool calls generated by the model, such as function calls."""


class ChatCompletionRequest(BaseModel):
    """Chat completion request model."""

    messages: list[ChatCompletionMessageParam] = Field(
        ...,
        description='List of chat messages',
    )
    model: str = Field(
        ...,
        description='Model to use for completion',
    )
    audio: ChatCompletionAudioParam | None = Field(
        default=None,
        description='Audio configuration for transcriptions or voice',
    )
    frequency_penalty: float | None = Field(
        default=None,
        description='Frequency penalty',
    )
    logit_bias: dict[str, int] | None = Field(
        default=None,
        description='Modify the likelihood of specified tokens',
    )
    logprobs: bool | None = Field(
        default=None,
        description='Whether to return log probabilities of tokens',
    )
    max_completion_tokens: int | None = Field(
        default=None,
        description='Maximum tokens for completion',
    )
    parallel_tool_calls: bool | None = Field(
        default=None,
        description='Allow execution of parallel tool calls',
    )
    presence_penalty: Number | None = Field(
        default=None,
        description='Presence penalty',
    )
    reasoning_effort: ChatCompletionReasoningEffort | None = Field(
        default=None,
        description='Reasoning level for the model',
    )
    seed: int | None = Field(
        default=None,
        description='Seed for deterministic completions',
    )
    stop: str | None = Field(
        default=None,
        description='Stop sequence(s) for generation',
    )
    stream: bool | None = Field(
        default=None,
        description='Whether to stream responses',
    )
    temperature: PositiveNumber | None = Field(
        default=None,
        description='Sampling temperature',
    )
    tools: list[ChatCompletionToolParam] | None = Field(
        default=None,
        description='List of tool definitions',
    )
    top_logprobs: Annotated[int, Field(ge=0, le=20)] | None = Field(
        default=None,
        description='Number of logprobs to return',
    )
    top_p: PositiveNumber | None = Field(
        default=None,
        description='Top-p sampling parameter',
    )
    user: str | None = Field(
        default=None,
        description='User identifier',
    )
    extra_headers: dict[str, str] | None = Field(
        default=None,
        description='Additional request headers',
    )
    extra_body: Any | None = Field(
        default=None,
        description='Additional request body fields',
    )

    model_config = ConfigDict(
        json_schema_extra={
            'examples': [
                {
                    'model': 'model-router',
                    'messages': [
                        {
                            'role': 'system',
                            'content': 'You are a helpful assistant.',
                        },
                        {
                            'role': 'user',
                            'content': 'Hi there.',
                        },
                    ],
                    'stream': False,
                },
            ],
        },
    )


class ResponsesRequest(BaseModel):
    """Responses API request model (OpenAI Responses endpoint).

    This mirrors the parameters accepted by the OpenAI Responses.create(...) method.
    """

    # Basic routing / control
    model: str = Field(..., description="Model ID used to generate the response, e.g. 'gpt-4o'")
    input: str | ResponseInputParam = Field(
        ...,
        description='Text, image, or file inputs to the model (string or SDK input param object)',
    )
    instructions: str | None = Field(
        default=None,
        description="A system/developer instruction to insert into the model's context",
    )
    prompt: ResponsePromptParam | None = Field(
        default=None,
        description='Reference to a prompt template and its variables (prompt param object)',
    )
    prompt_cache_key: str | None = Field(
        default=None,
        description='Cache key used by OpenAI for prompt caching (replaces `user` in some flows)',
    )
    conversation: Any | None = Field(
        default=None,
        description='Conversation reference whose items are prepended to `input` for this request',
    )
    previous_response_id: str | None = Field(
        default=None,
        description='ID of a previous response to continue a multi-turn conversation',
    )

    # Sampling / output control
    temperature: PositiveNumber | None = Field(
        default=None,
        description='Sampling temperature (0.0 - 2.0)',
    )
    top_p: PositiveNumber | None = Field(
        default=None,
        description='Nucleus sampling top_p (0.0 - 2.0)',
    )
    top_logprobs: Annotated[int, Field(ge=0, le=20)] | None = Field(
        default=None,
        description='Number of top logprobs to return',
    )
    max_output_tokens: int | None = Field(
        default=None,
        description='Upper bound for generated tokens (includes visible + reasoning tokens)',
    )
    truncation: Literal['auto', 'disabled'] | None = Field(
        default=None,
        description='Truncation strategy if input exceeds model context window',
    )

    # Tools / function-calling / reasoning
    tools: list[ToolParam] | None = Field(
        default=None,
        description='List/definitions of tools the model may call (SDK ToolParam objects)',
    )
    tool_choice: Any | None = Field(
        default=None,
        description='How the model should select tools (tool choice param)',
    )
    max_tool_calls: int | None = Field(
        default=None,
        description='Max total tool calls allowed for the response',
    )
    parallel_tool_calls: bool | None = Field(
        default=True,
        description='Allow parallel execution of tool calls',
    )
    reasoning: Reasoning | None = Field(
        default=None,
        description='Reasoning configuration (for reasoning-capable models)',
    )

    # Misc / safety / storage
    user: str | None = Field(
        default=None,
        description='User identifier (being replaced by `safety_identifier` / `prompt_cache_key`)',
    )
    safety_identifier: str | None = Field(
        default=None,
        description='Stable identifier to help detect policy abuse (hash username/email)',
    )
    store: bool | None = Field(
        default=True,
        description='Whether to store the generated model response for later retrieval',
    )
    service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | None = Field(
        default='auto',
        description='Processing tier for serving the request',
    )

    # Streaming
    stream: bool | None = Field(
        default=False,
        description='Whether to stream response data (server-sent events)',
    )
    stream_options: Any | None = Field(
        default=None,
        description='Streaming options (only used when `stream` is True)',
    )

    # Text / structured output config
    text: Any | None = Field(
        default=None,
        description='Configuration for text output (plain or structured JSON)',
    )

    # Metadata / logging / headers
    metadata: dict[str, str] | None = Field(
        default=None,
        description='Optional set of up to 16 key/value pairs attached to this request',
    )
    include: list[ResponseIncludable] | None = Field(
        default=None,
        description="Additional output data to include (e.g. 'message.output_text.logprobs')",
    )
    extra_headers: Any | None = Field(
        default=None,
        description='Extra headers to send with the request',
    )
    extra_body: Any | None = Field(
        default=None,
        description='Additional JSON properties to include in the request body',
    )
    extra_query: Any | None = Field(
        default=None,
        description='Additional Query properties to include with the request ',
    )

    # Misc / legacy / fine-grained
    background: bool | None = Field(
        default=False,
        description='Whether to run the model response in the background',
    )

    model_config = ConfigDict(
        json_schema_extra={
            'examples': [
                {
                    'model': 'gpt-5-mini',
                    'instructions': 'You are a helpful assistant.',
                    'input': [
                        {'role': 'user', 'content': 'Hello!'},
                    ],
                    'stream': False,
                },
            ],
        },
        arbitrary_types_allowed=True,
    )


# Model returning list of OpenAI Models
class ModelsResponse(BaseModel):
    """Models list response."""

    object: str = Field(default='list', description='Object type')
    data: list[Model] = Field(..., description='List of available models')


# Simple Model for OpenAI ErrorObject response
class ErrorResponse(BaseModel):
    """Error response model."""

    error: ErrorObject = Field(..., description='Error details')
